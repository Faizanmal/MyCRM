# Secure Production Docker Compose Configuration
# IMPORTANT: Set all required environment variables in .env.production
version: '3.8'

services:
  # PostgreSQL Database - Secure Configuration
  db:
    image: postgres:15-alpine
    container_name: mycrm_db_prod
    environment:
      POSTGRES_DB: ${DATABASE_NAME:?Database name required}
      POSTGRES_USER: ${DATABASE_USER:?Database user required}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD:?Database password required}
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=en_US.UTF-8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups:ro
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c max_connections=100
      -c log_connections=on
      -c log_disconnections=on
      -c log_line_prefix='%t [%p]: [%l-1] user=%u,db=%d '
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DATABASE_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis - Secure with Password
  redis:
    image: redis:7-alpine
    container_name: mycrm_redis_prod
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:?Redis password required}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--pass", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Backend Application - Production Mode
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.production
    container_name: mycrm_backend_prod
    command: >
      gunicorn backend.wsgi:application
      --bind 0.0.0.0:8000
      --workers 4
      --threads 2
      --worker-class gthread
      --timeout 120
      --graceful-timeout 30
      --max-requests 1000
      --max-requests-jitter 50
      --access-logfile -
      --error-logfile -
      --log-level info
    volumes:
      - static_volume:/app/staticfiles:ro
      - media_volume:/app/media
      - ./logs:/app/logs
    env_file:
      - ./backend/.env.production
    expose:
      - "8000"
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
      - frontend
    restart: always
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8000/api/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Celery Worker
  celery:
    build:
      context: ./backend
      dockerfile: Dockerfile.production
    container_name: mycrm_celery_prod
    command: celery -A backend worker -l info --concurrency=4 --max-tasks-per-child=1000
    volumes:
      - media_volume:/app/media
      - ./logs:/app/logs
    env_file:
      - ./backend/.env.production
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Celery Beat Scheduler
  celery-beat:
    build:
      context: ./backend
      dockerfile: Dockerfile.production
    container_name: mycrm_celery_beat_prod
    command: celery -A backend beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    volumes:
      - ./logs:/app/logs
    env_file:
      - ./backend/.env.production
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: mycrm_nginx_prod
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - static_volume:/usr/share/nginx/html/static:ro
      - media_volume:/usr/share/nginx/html/media:ro
      - ./nginx/logs:/var/log/nginx
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - frontend
    restart: always
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Frontend Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.production
    container_name: mycrm_frontend_prod
    environment:
      - NEXT_PUBLIC_API_URL=https://${DOMAIN}/api
      - NODE_ENV=production
    expose:
      - "3000"
    networks:
      - frontend
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  static_volume:
    driver: local
  media_volume:
    driver: local

networks:
  backend:
    driver: bridge
    internal: true  # Backend network isolated from internet
  frontend:
    driver: bridge
